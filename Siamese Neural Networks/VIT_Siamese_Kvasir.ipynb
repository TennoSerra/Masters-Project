{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37285e3-1044-4b05-b03c-c9bd392a50a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\MP_Siamese\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTMSNForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c33f5ba-50fb-4c0a-a7eb-84814b9dc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging and device configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623cf970-79dc-4776-a3a2-5d41f3e77c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"models\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca8faf7-bb47-4e0c-9435-c7b6d1ed9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading utilities\n",
    "def save_model(model, path, optimizer=None, epoch=None, loss=None):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    if optimizer:\n",
    "        save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    if epoch is not None:\n",
    "        save_dict['epoch'] = epoch\n",
    "    if loss is not None:\n",
    "        save_dict['loss'] = loss\n",
    "    \n",
    "    torch.save(save_dict, path)\n",
    "    logger.info(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797c8c55-442b-4111-909b-5909f91b820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.processor = ViTImageProcessor.from_pretrained('facebook/vit-msn-small', do_rescale=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "                img = img.numpy()\n",
    "            inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "            return inputs['pixel_values'].squeeze(), self.labels[idx]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {self.image_paths[idx]}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "def prepare_data(data_dir):\n",
    "    try:\n",
    "        classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        if 'polyps' in classes:\n",
    "            classes.remove('polyps')\n",
    "        print(f\"Number of classes: {len(classes)}\")\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for idx, class_name in enumerate(classes):\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                          if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            image_paths.extend(class_images)\n",
    "            labels.extend([idx] * len(class_images))\n",
    "        \n",
    "        logger.info(f\"Found {len(image_paths)} images across {len(classes)} classes\")\n",
    "        return train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf56f0ba-9651-4fee-b8cd-4fa3b60d8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        \n",
    "    def stratified_split(self, test_size=80):\n",
    "        # Collect all image paths and labels\n",
    "        all_image_paths = {}\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                            if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_image_paths[class_name] = class_images\n",
    "        \n",
    "        # Stratified test set selection\n",
    "        test_paths = []\n",
    "        test_labels = []\n",
    "        remaining_paths = {}\n",
    "        \n",
    "        for idx, (class_name, images) in enumerate(all_image_paths.items()):\n",
    "            # Calculate test images per class (proportional to 100 total)\n",
    "            class_test_size = max(1, test_size // len(self.classes))\n",
    "            \n",
    "            # Randomly select test images\n",
    "            test_class_images = random.sample(images, min(class_test_size, len(images)))\n",
    "            \n",
    "            # Add to test set\n",
    "            test_paths.extend(test_class_images)\n",
    "            test_labels.extend([idx] * len(test_class_images))\n",
    "            \n",
    "            # Remove test images from original set\n",
    "            remaining_images = [img for img in images if img not in test_class_images]\n",
    "            remaining_paths[class_name] = remaining_images\n",
    "        \n",
    "        # Separate polyps class\n",
    "        polyps_paths = all_image_paths.get('polyps', [])\n",
    "        \n",
    "        # Prepare remaining paths for train/val split\n",
    "        all_remaining_paths = []\n",
    "        all_remaining_labels = []\n",
    "        for class_name, paths in remaining_paths.items():\n",
    "            if class_name != 'polyps':\n",
    "                all_remaining_paths.extend(paths)\n",
    "                all_remaining_labels.extend([self.classes.index(class_name)] * len(paths))\n",
    "        \n",
    "        # Split remaining images into train and validation\n",
    "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "            all_remaining_paths, all_remaining_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'test_paths': test_paths,\n",
    "            'test_labels': test_labels,\n",
    "            'train_paths': train_paths,\n",
    "            'train_labels': train_labels,\n",
    "            'val_paths': val_paths,\n",
    "            'val_labels': val_labels,\n",
    "            'polyps_paths': polyps_paths\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaf1762-1b21-4dee-a19b-f8444a5684cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333a6c06-ebed-4b2f-810f-85db415b7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "DATA_DIR = r\"..\\Datasets\\kvasir-dataset-v2\"\n",
    "# Initialize dataset manager\n",
    "dataset_manager = DatasetManager(DATA_DIR)\n",
    "# Perform stratified splitting\n",
    "split_data = dataset_manager.stratified_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b89aa13-ca8b-402c-888e-78cbeda13efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.vit = ViTMSNForImageClassification.from_pretrained('facebook/vit-msn-small')\n",
    "        \n",
    "        # Dynamically determine number of classes if not provided\n",
    "        if num_classes is None:\n",
    "            num_classes = len(set(split_data['train_labels']))\n",
    "        \n",
    "        self.fc = nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        outputs = self.vit(x, output_hidden_states=True)\n",
    "        return self.fc(outputs.hidden_states[-1][:, 0])\n",
    "        \n",
    "    def forward(self, x1, x2=None):\n",
    "        output1 = self.forward_one(x1)\n",
    "        if x2 is not None:\n",
    "            output2 = self.forward_one(x2)\n",
    "            return output1, output2\n",
    "        return output1\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dad7e7e-132c-4151-a086-22cb7d9927e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Testing function\n",
    "def test_model(model, test_loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        accuracy = 100. * correct / total\n",
    "        logger.info(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9c0724-75b7-4f8a-8f1a-5c4f0c063d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiments(model, test_loader, polyps_paths, max_shots=10):\n",
    "    results = []\n",
    "    \n",
    "    for shots in range(1, max_shots + 1):\n",
    "        # Reset model and fine-tune\n",
    "        fine_tuned_model = few_shot_fine_tuning(model, polyps_paths, num_shots=shots)\n",
    "        \n",
    "        # Test fine-tuned model\n",
    "        test_accuracy = test_model(fine_tuned_model, test_loader)\n",
    "        \n",
    "        results.append({\n",
    "            'Shots': shots,\n",
    "            'Accuracy': test_accuracy\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(os.path.join(SAVE_DIR, 'few_shot_results.csv'), index=False)\n",
    "    \n",
    "    # Plot line chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['Shots'], results_df['Accuracy'], marker='o')\n",
    "    plt.title('Few-Shot Learning: Model Performance')\n",
    "    plt.xlabel('Number of Shots')\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'few_shot_performance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2286d95-78dc-4169-b74c-b2353d8f0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, save_path, num_epochs=10):\n",
    "    # Check if model already exists\n",
    "    if load_model(model, save_path):\n",
    "        return model\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_model(model, save_path, optimizer, epoch, best_val_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def few_shot_fine_tuning(model, polyps_paths, num_shots=5):\n",
    "    fine_tune_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Prepare save path\n",
    "    save_path = f\"models/fine_tuned_{num_shots}.pth\"\n",
    "    \n",
    "    # Check if fine-tuned model exists\n",
    "    if load_model(model, save_path):\n",
    "        return model\n",
    "    \n",
    "    # Select a small number of polyps images\n",
    "    selected_polyps = random.sample(polyps_paths, min(num_shots, len(polyps_paths)))\n",
    "    \n",
    "    # Create a custom dataset for fine-tuning\n",
    "    fine_tune_dataset = KvasirDataset(selected_polyps, [0]*len(selected_polyps), fine_tune_transform)\n",
    "    fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=num_shots, shuffle=True)\n",
    "    \n",
    "    # Prepare for fine-tuning\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Fine-tuning loop\n",
    "    for _ in range(10):  # Few iterations for few-shot learning\n",
    "        for images, _ in fine_tune_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = torch.zeros(images.size(0), dtype=torch.long).to(DEVICE)  # Polyps class\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    save_model(model, save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7c5fc7-0c79-4701-ace5-b88807b9ebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (4.55.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in g:\\anaconda\\envs\\mp_siamese\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61406c8-0249-4f35-aa49-51ba6353cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf134202-0b19-41b6-b989-8b069b14239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTMSNForImageClassification were not initialized from the model checkpoint at facebook/vit-msn-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ninja\\AppData\\Local\\Temp\\ipykernel_76700\\3008922499.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n",
      "INFO:__main__:Model loaded from models/main_model.pth\n",
      "INFO:__main__:Test Accuracy: 83.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_1.pth\n",
      "INFO:__main__:Test Accuracy: 75.00%\n",
      "INFO:__main__:Model saved to models/fine_tuned_2.pth\n",
      "INFO:__main__:Test Accuracy: 60.00%\n",
      "INFO:__main__:Model saved to models/fine_tuned_3.pth\n",
      "INFO:__main__:Test Accuracy: 52.50%\n",
      "INFO:__main__:Model saved to models/fine_tuned_4.pth\n",
      "INFO:__main__:Test Accuracy: 33.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_5.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_6.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_7.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_8.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_9.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n",
      "INFO:__main__:Model saved to models/fine_tuned_10.pth\n",
      "INFO:__main__:Test Accuracy: 13.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Shots  Accuracy\n",
      "0      1     75.00\n",
      "1      2     60.00\n",
      "2      3     52.50\n",
      "3      4     33.75\n",
      "4      5     13.75\n",
      "5      6     13.75\n",
      "6      7     13.75\n",
      "7      8     13.75\n",
      "8      9     13.75\n",
      "9     10     13.75\n"
     ]
    }
   ],
   "source": [
    "def main():  \n",
    "    # Create datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = KvasirDataset(split_data['train_paths'], split_data['train_labels'], transform)\n",
    "    val_dataset = KvasirDataset(split_data['val_paths'], split_data['val_labels'], transform)\n",
    "    test_dataset = KvasirDataset(split_data['test_paths'], split_data['test_labels'], transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Model initialization and training\n",
    "    model = SiameseNetwork().to(DEVICE)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # Training loop\n",
    "    train_model(model, train_loader, val_loader, \"models/main_model.pth\")   \n",
    "    \n",
    "    # Perform testing\n",
    "    test_accuracy = test_model(model, test_loader)\n",
    "    \n",
    "    # Few-shot fine-tuning with polyps\n",
    "    # Run few-shot experiments\n",
    "    few_shot_results = run_few_shot_experiments(\n",
    "        model, \n",
    "        test_loader, \n",
    "        split_data['polyps_paths'], \n",
    "        max_shots=10\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(few_shot_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da8810-723c-488e-9ee2-ee15fb0f9ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
