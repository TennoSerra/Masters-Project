{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37285e3-1044-4b05-b03c-c9bd392a50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTMSNForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33f5ba-50fb-4c0a-a7eb-84814b9dc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging and device configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cf970-79dc-4776-a3a2-5d41f3e77c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = r\"..\\Datasets\\kvasir-dataset-v2\"\n",
    "SAVE_DIR = \"models\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8faf7-bb47-4e0c-9435-c7b6d1ed9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading utilities\n",
    "def save_model(model, path, optimizer=None, epoch=None, loss=None):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    if optimizer:\n",
    "        save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    if epoch is not None:\n",
    "        save_dict['epoch'] = epoch\n",
    "    if loss is not None:\n",
    "        save_dict['loss'] = loss\n",
    "    \n",
    "    torch.save(save_dict, path)\n",
    "    logger.info(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c8c55-442b-4111-909b-5909f91b820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.processor = ViTImageProcessor.from_pretrained('facebook/vit-msn-small', do_rescale=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "                img = img.numpy()\n",
    "            inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "            return inputs['pixel_values'].squeeze(), self.labels[idx]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {self.image_paths[idx]}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "def prepare_data(data_dir):\n",
    "    try:\n",
    "        classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        if 'polyps' in classes:\n",
    "            classes.remove('polyps')\n",
    "        print(f\"Number of classes: {len(classes)}\")\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for idx, class_name in enumerate(classes):\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                          if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            image_paths.extend(class_images)\n",
    "            labels.extend([idx] * len(class_images))\n",
    "        \n",
    "        logger.info(f\"Found {len(image_paths)} images across {len(classes)} classes\")\n",
    "        return train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56f0ba-9651-4fee-b8cd-4fa3b60d8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        \n",
    "    def stratified_split(self, test_size=80):\n",
    "        # Collect all image paths and labels\n",
    "        all_image_paths = {}\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                            if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_image_paths[class_name] = class_images\n",
    "        \n",
    "        # Stratified test set selection\n",
    "        test_paths = []\n",
    "        test_labels = []\n",
    "        remaining_paths = {}\n",
    "        \n",
    "        for idx, (class_name, images) in enumerate(all_image_paths.items()):\n",
    "            # Calculate test images per class (proportional to 100 total)\n",
    "            class_test_size = max(1, test_size // len(self.classes))\n",
    "            \n",
    "            # Randomly select test images\n",
    "            test_class_images = random.sample(images, min(class_test_size, len(images)))\n",
    "            \n",
    "            # Add to test set\n",
    "            test_paths.extend(test_class_images)\n",
    "            test_labels.extend([idx] * len(test_class_images))\n",
    "            \n",
    "            # Remove test images from original set\n",
    "            remaining_images = [img for img in images if img not in test_class_images]\n",
    "            remaining_paths[class_name] = remaining_images\n",
    "        \n",
    "        # Separate polyps class\n",
    "        polyps_paths = all_image_paths.get('polyps', [])\n",
    "        \n",
    "        # Prepare remaining paths for train/val split\n",
    "        all_remaining_paths = []\n",
    "        all_remaining_labels = []\n",
    "        for class_name, paths in remaining_paths.items():\n",
    "            if class_name != 'polyps':\n",
    "                all_remaining_paths.extend(paths)\n",
    "                all_remaining_labels.extend([self.classes.index(class_name)] * len(paths))\n",
    "        \n",
    "        # Split remaining images into train and validation\n",
    "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "            all_remaining_paths, all_remaining_labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'test_paths': test_paths,\n",
    "            'test_labels': test_labels,\n",
    "            'train_paths': train_paths,\n",
    "            'train_labels': train_labels,\n",
    "            'val_paths': val_paths,\n",
    "            'val_labels': val_labels,\n",
    "            'polyps_paths': polyps_paths\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf1762-1b21-4dee-a19b-f8444a5684cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89aa13-ca8b-402c-888e-78cbeda13efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.vit = ViTMSNForImageClassification.from_pretrained('facebook/vit-msn-small')\n",
    "        \n",
    "        # Dynamically determine number of classes if not provided\n",
    "        if num_classes is None:\n",
    "            num_classes = len(set(split_data['train_labels']))\n",
    "        \n",
    "        self.fc = nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        outputs = self.vit(x, output_hidden_states=True)\n",
    "        return self.fc(outputs.hidden_states[-1][:, 0])\n",
    "        \n",
    "    def forward(self, x1, x2=None):\n",
    "        output1 = self.forward_one(x1)\n",
    "        if x2 is not None:\n",
    "            output2 = self.forward_one(x2)\n",
    "            return output1, output2\n",
    "        return output1\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2286d95-78dc-4169-b74c-b2353d8f0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, save_path, num_epochs=10):\n",
    "    # Check if model already exists\n",
    "    if load_model(model, save_path):\n",
    "        return model\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_model(model, save_path, optimizer, epoch, best_val_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def few_shot_fine_tuning(model, polyps_paths, num_shots=5):\n",
    "    fine_tune_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Prepare save path\n",
    "    save_path = f\"models/fine_tuned_{num_shots}.pth\"\n",
    "    \n",
    "    # Check if fine-tuned model exists\n",
    "    if load_model(model, save_path):\n",
    "        return model\n",
    "    \n",
    "    # Select a small number of polyps images\n",
    "    selected_polyps = random.sample(polyps_paths, min(num_shots, len(polyps_paths)))\n",
    "    \n",
    "    # Create a custom dataset for fine-tuning\n",
    "    fine_tune_dataset = KvasirDataset(selected_polyps, [0]*len(selected_polyps), fine_tune_transform)\n",
    "    fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=num_shots, shuffle=True)\n",
    "    \n",
    "    # Prepare for fine-tuning\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Fine-tuning loop\n",
    "    for _ in range(10):  # Few iterations for few-shot learning\n",
    "        for images, _ in fine_tune_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = torch.zeros(images.size(0), dtype=torch.long).to(DEVICE)  # Polyps class\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    save_model(model, save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf134202-0b19-41b6-b989-8b069b14239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset directory\n",
    "    DATA_DIR = r\"..\\Datasets\\kvasir-dataset-v2\"\n",
    "    \n",
    "    # Initialize dataset manager\n",
    "    dataset_manager = DatasetManager(DATA_DIR)\n",
    "    \n",
    "    # Perform stratified splitting\n",
    "    split_data = dataset_manager.stratified_split()\n",
    "    \n",
    "    # Create datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = KvasirDataset(split_data['train_paths'], split_data['train_labels'], transform)\n",
    "    val_dataset = KvasirDataset(split_data['val_paths'], split_data['val_labels'], transform)\n",
    "    test_dataset = KvasirDataset(split_data['test_paths'], split_data['test_labels'], transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Model initialization and training\n",
    "    model = SiameseNetwork().to(DEVICE)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # Training loop\n",
    "    train_model(model, train_loader, val_loader, \"models/main_model.pth\")   \n",
    "    # Testing function\n",
    "    def test_model(model, test_loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / total\n",
    "        logger.info(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "        return accuracy\n",
    "    \n",
    "    # Perform testing\n",
    "    test_accuracy = test_model(model, test_loader)\n",
    "    \n",
    "    # Few-shot fine-tuning with polyps\n",
    "    fine_tuned_model = few_shot_fine_tuning(model, split_data['polyps_paths'], num_shots=5)\n",
    "    \n",
    "    # Test fine-tuned model\n",
    "    fine_tuned_accuracy = test_model(fine_tuned_model, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da8810-723c-488e-9ee2-ee15fb0f9ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MP_Siamese]",
   "language": "python",
   "name": "conda-env-MP_Siamese-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
