{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715729bb-8ae1-46c0-8706-9fb228eb5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\YOLO\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import logging\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0607df3-eb18-4c53-aa1e-d47348a4ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd19f7a4-ef6e-4c64-aa0f-ed5295aac1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging and device configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-4\n",
    "DATA_DIR =  r\"..\\Datasets\\kvasir-dataset-v2\"\n",
    "SAVE_DIR = \"vit_models\"\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148a175a-56b5-4a48-a229-83fbbe62e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\",do_rescale=False)\n",
    "        return inputs['pixel_values'].squeeze(), self.labels[idx]\n",
    "\n",
    "def prepare_dataset(data_dir):\n",
    "    # Collect image paths and labels\n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                        if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        image_paths.extend(class_images)\n",
    "        labels.extend([idx] * len(class_images))\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    "    )\n",
    "    print(f\"Dataset Prepared!\")\n",
    "    return {\n",
    "        'train_paths': train_paths,\n",
    "        'train_labels': train_labels,\n",
    "        'val_paths': val_paths,\n",
    "        'val_labels': val_labels,\n",
    "        'test_paths': test_paths,\n",
    "        'test_labels': test_labels,\n",
    "        'classes': classes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e2ad5b-42e2-4fff-a3dc-c47e5bb4dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_model(model, train_loader, val_loader, classes, patience=3):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logger.info(f\"Existing model found at {MODEL_PATH}. Skipping training.\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        return model\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Train]')\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{train_loss/len(train_loader):.4f}'})\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/len(val_loader):.4f}',\n",
    "                    'acc': f'{100*correct/total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss/len(val_loader)\n",
    "        early_stopping(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d99387-d249-45f7-9013-8917863ead44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    test_pbar = tqdm(test_loader, desc='Testing')\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds, target_names=classes, digits=4)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Performance Visualization\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, class_accuracy * 100)\n",
    "    plt.title('Accuracy per Class')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'class_performance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adcfb154-57e0-407b-bcca-8accbe9e9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [02:50<00:00,  3.75it/s, loss=0.4585]\n",
      "Epoch 1/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:34<00:00,  4.59it/s, loss=0.2676, acc=90.62%]\n",
      "Epoch 2/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [02:48<00:00,  3.79it/s, loss=0.2851]\n",
      "Epoch 2/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:34<00:00,  4.66it/s, loss=0.2924, acc=89.30%]\n",
      "Epoch 3/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [02:53<00:00,  3.69it/s, loss=0.2150]\n",
      "Epoch 3/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:34<00:00,  4.59it/s, loss=0.2971, acc=89.22%]\n",
      "Epoch 4/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [02:53<00:00,  3.68it/s, loss=0.1553]\n",
      "Epoch 4/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:34<00:00,  4.65it/s, loss=0.2809, acc=90.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 4 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:12<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "    dyed-lifted-polyps     0.9038    0.9400    0.9216       200\n",
      "dyed-resection-margins     0.9442    0.9300    0.9370       200\n",
      "           esophagitis     0.8077    0.8400    0.8235       200\n",
      "          normal-cecum     0.8622    0.9700    0.9129       200\n",
      "        normal-pylorus     0.9948    0.9600    0.9771       200\n",
      "         normal-z-line     0.8283    0.8200    0.8241       200\n",
      "                polyps     0.8788    0.8700    0.8744       200\n",
      "    ulcerative-colitis     0.9364    0.8100    0.8686       200\n",
      "\n",
      "              accuracy                         0.8925      1600\n",
      "             macro avg     0.8945    0.8925    0.8924      1600\n",
      "          weighted avg     0.8945    0.8925    0.8924      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset(DATA_DIR)\n",
    "    \n",
    "    # Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = KvasirDataset(dataset['train_paths'], dataset['train_labels'], transform)\n",
    "    val_dataset = KvasirDataset(dataset['val_paths'], dataset['val_labels'], transform)\n",
    "    test_dataset = KvasirDataset(dataset['test_paths'], dataset['test_labels'], transform)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        num_labels=len(dataset['classes']),\n",
    "        attn_implementation=\"sdpa\",\n",
    "        torch_dtype=torch.float32,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Train or load the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, dataset['classes'])\n",
    "    \n",
    "    # Test the model\n",
    "    test_model(trained_model, test_loader, dataset['classes'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f95858-599d-4775-ae7a-f14db0a0f16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
