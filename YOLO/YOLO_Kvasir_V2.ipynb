{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac98982-9784-4e9b-8338-cceba53341d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\YOLO\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd2ba51-499f-403d-ad09-ade3ce57dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging and device configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-4\n",
    "DATA_DIR = r\"G:\\USER\\Documents\\GitHub\\Masters-Project\\Datasets\\kvasir-dataset-v2\"\n",
    "SAVE_DIR = \"medical_models\"\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd0e623-2525-44b1-bb83-9b4c5aa9e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIDetector:\n",
    "    def __init__(self, min_area=1000):\n",
    "        \"\"\"\n",
    "        Initialize ROI detector with OpenCV contour-based approach\n",
    "        \n",
    "        Args:\n",
    "            min_area (int): Minimum area of contour to be considered a valid region\n",
    "        \"\"\"\n",
    "        self.min_area = min_area\n",
    "    \n",
    "    def detect_roi(self, image):\n",
    "        \"\"\"\n",
    "        Detect regions of interest using contour detection\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Input image\n",
    "        \n",
    "        Returns:\n",
    "            list of tuples: Bounding boxes of ROIs [(x, y, w, h), ...]\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply Gaussian blur\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        \n",
    "        # Threshold the image\n",
    "        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Filter contours by area\n",
    "        valid_contours = [\n",
    "            cv2.boundingRect(cnt) for cnt in contours \n",
    "            if cv2.contourArea(cnt) >= self.min_area\n",
    "        ]\n",
    "        \n",
    "        return valid_contours\n",
    "    \n",
    "    def crop_to_roi(self, image, roi):\n",
    "        \"\"\"\n",
    "        Crop image to specified region of interest\n",
    "        \n",
    "        Args:\n",
    "            image (np.ndarray): Original image\n",
    "            roi (tuple): Bounding box (x, y, w, h)\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Cropped image\n",
    "        \"\"\"\n",
    "        x, y, w, h = roi\n",
    "        return image[y:y+h, x:x+w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed7a914-046a-450f-ab68-d8c6b3e5226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.roi_detector = ROIDetector()\n",
    "        \n",
    "        # Augmentation techniques\n",
    "        self.augmentation = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Blur(blur_limit=3, p=0.2),\n",
    "        ])\n",
    "        \n",
    "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect ROIs\n",
    "        rois = self.roi_detector.detect_roi(img)\n",
    "        \n",
    "        # If ROIs found, crop to the largest region\n",
    "        if rois:\n",
    "            # Sort ROIs by area and select the largest\n",
    "            largest_roi = max(rois, key=lambda r: r[2] * r[3])\n",
    "            img = self.roi_detector.crop_to_roi(img, largest_roi)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            augmented = self.augmentation(image=img)\n",
    "            img = augmented['image']\n",
    "        \n",
    "        # Process for ViT\n",
    "        inputs = self.processor(images=img, return_tensors=\"pt\", do_rescale=False)\n",
    "        \n",
    "        return inputs['pixel_values'].squeeze(), self.labels[idx]\n",
    "\n",
    "def prepare_dataset(data_dir):\n",
    "    # Collect image paths and labels\n",
    "    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        class_images = [os.path.join(class_path, img) for img in os.listdir(class_path) \n",
    "                        if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        image_paths.extend(class_images)\n",
    "        labels.extend([idx] * len(class_images))\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'train_paths': train_paths,\n",
    "        'train_labels': train_labels,\n",
    "        'val_paths': val_paths,\n",
    "        'val_labels': val_labels,\n",
    "        'test_paths': test_paths,\n",
    "        'test_labels': test_labels,\n",
    "        'classes': classes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33caab25-2ee2-4c43-aaf7-1cb151eeb35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780c66ac-41fa-4f40-9231-0818e7df4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit_model(model, train_loader, val_loader, classes, patience=3):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logger.info(f\"Existing model found at {MODEL_PATH}. Skipping training.\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        return model\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Train]')\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{train_loss/len(train_loader):.4f}'})\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Val]')\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/len(val_loader):.4f}',\n",
    "                    'acc': f'{100*correct/total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss/len(val_loader)\n",
    "        early_stopping(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b1aa6b-12b5-49e7-ad12-c43e4423ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    test_pbar = tqdm(test_loader, desc='Testing')\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_pbar:\n",
    "            images_for_vit = images.to(DEVICE)\n",
    "            outputs = model(images_for_vit).logits\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    # Classification Report   \n",
    "    report = classification_report(all_labels, all_preds, target_names=classes, digits=4)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e289f9f-d9e9-4bdc-8b23-6df0d7a18c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [04:46<00:00,  2.24it/s, loss=1.0379]\n",
      "Epoch 1/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [01:02<00:00,  2.55it/s, loss=0.7983, acc=59.06%]\n",
      "Epoch 2/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [04:27<00:00,  2.40it/s, loss=0.8137]\n",
      "Epoch 2/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [01:00<00:00,  2.63it/s, loss=0.6906, acc=68.44%]\n",
      "Epoch 3/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [03:58<00:00,  2.69it/s, loss=0.7467]\n",
      "Epoch 3/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:51<00:00,  3.08it/s, loss=0.7135, acc=66.17%]\n",
      "Epoch 4/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [03:50<00:00,  2.77it/s, loss=0.7248]\n",
      "Epoch 4/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:52<00:00,  3.03it/s, loss=0.7312, acc=67.89%]\n",
      "Epoch 5/10 [Train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 640/640 [03:48<00:00,  2.80it/s, loss=0.6897]\n",
      "Epoch 5/10 [Val]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:49<00:00,  3.20it/s, loss=0.7669, acc=63.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:19<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "    dyed-lifted-polyps     0.8182    0.3600    0.5000       200\n",
      "dyed-resection-margins     0.6080    0.9150    0.7305       200\n",
      "           esophagitis     0.5686    0.7250    0.6374       200\n",
      "          normal-cecum     0.9296    0.3300    0.4871       200\n",
      "        normal-pylorus     0.9401    0.7850    0.8556       200\n",
      "         normal-z-line     0.6541    0.6050    0.6286       200\n",
      "                polyps     0.4136    0.8850    0.5637       200\n",
      "    ulcerative-colitis     0.7238    0.3800    0.4984       200\n",
      "\n",
      "              accuracy                         0.6231      1600\n",
      "             macro avg     0.7070    0.6231    0.6126      1600\n",
      "          weighted avg     0.7070    0.6231    0.6126      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset(DATA_DIR)\n",
    "    \n",
    "    # Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MedicalImageDataset(dataset['train_paths'], dataset['train_labels'], transform)\n",
    "    val_dataset = MedicalImageDataset(dataset['val_paths'], dataset['val_labels'], transform)\n",
    "    test_dataset = MedicalImageDataset(dataset['test_paths'], dataset['test_labels'], transform)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize ViT model\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        num_labels=len(dataset['classes']),\n",
    "        attn_implementation=\"sdpa\", \n",
    "        torch_dtype=torch.float32,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Train or load the ViT model\n",
    "    trained_model = train_vit_model(model, train_loader, val_loader, dataset['classes'])\n",
    "    \n",
    "    # Test the model\n",
    "    test_model(trained_model, test_loader, dataset['classes'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656abd6-dc71-4c73-ae66-f45229e36474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1056a-0b20-48f7-b33e-cf3c310d6626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
